고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 05
중고등학교 교과서(https://goo.gl/dtHgWy)
함수(삼각함수)
행렬, 복소수
방정식
미분

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 06
머신러닝
인공지능
딥러닝
텐서플로우

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 07/08
인공지능 구현방법
합리주의 : Top-Down
터득한 지식을 사람이 컴퓨터에게 제공

경험주의 : Bottom-Up
컴퓨터가 데이터로부터 지식을 경험적으로 습득 신경망, 딥러닝

Ex) 가로등을 킬 때
합리주의 - 일정시간에 켜지게 한다.
         밝기 센서의 값이 12.45에 켜지게 한다.
경험주의 - 사람이 누르면 켜지게 한다.
         그때의 시간과 주변 밝기를 저장해둔다
		 저장된 값을 이용, 최대 효과를 찾는다.

지도학습 : 데이터마다 답(레이블)이 있는 경우
비지도학습 : 데이터가 답을 갖지 않은 경우
강화학습 : 시간이 지난 다음에 답이 나오는 경우

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 09
python의 package인 virtualenv을 이용해서 가상환경을 만듬
virtualenv -p 파이썬.exe실행경로 만들고자하는가상개발환경이름
특정 폴더로 이동 후,
ex) virtualenv -p c:\Python36\python.exe tf36

이렇게 하면 C에 설치된 Python을 사용하는 것이 아니라, 가상환경에 있는 Python을 사용할 수 있음
--> 버전 문제, 패키지 충돌 문제를 피할 수 있음

가상개발환경 실행
tf36\script\activate를 실행

결과)
(tf36) 폴더 이름 >

각종 패키지 설치
> pip install scipy matplotlib tensorflow jupyter

all.bat
@echo
atom
cmd /k .\tf36\scrips\activate

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 10
설치된 package 확인
> pip list

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 15

numpy 실습
numpy 브로드캐스트(A,B의 행렬이 다를 경우 작은 값에 맞춰준다.)

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 16
matplotlib을 이용하는 방법 강의
jupyter notebook을 이용하면 생각보다 재미있음

가상환경에서 > jupyter notebook
chrome으로 연결 후 New - python3로 python cmd 화면 열어서 편집

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 18

simple Neural Network
-> Hidden Layer가 1개, 1개 가지고는 제약조건이 많음
-> 신경망
-> Deep Learning 이전에 사용되던 방법

Deep Learning Neural Network
-> Hidden Layer가 다수 존재

Hidden Layer가 뭘 하는지 우리는 잘 알지 못함
Hidden Layer이 있어야지만 학습이 일어나 결과(output Layer)가 만들어짐

Ex) 아주 작은 그림이 숫자가 1인지, 2인지 판별할 때
가로X세로(100x100) => 10000 pixel / 각 점들은 고유의 값을 가지고 있음

입력값이 10000개가 되고, Hidden Layer가 몇개가 될지 알수는 없지만, 사용자가 적당히 정할 수 있음
실험 or 경험으로 Hidden Layer를 이정도 했을 때 괜찮은 결과가 나온다는 논문이 계속해서 나오고 있음

output Layer는 0 혹은 1로 나올 수도 있고, 여러 개로 나올 수도 있음

퍼셉트론
1957년에 나온 개념
신경망은 퍼셉트론을 기반으로 만들어짐
들어오는 여러가지 입력값에 가중치를 곱해 결과값을 특정 값을 넘느냐 여부에 따라서 0,1로 판단
참고 자료 : https://sacko.tistory.com/10
퍼셉트론은 2진(0,1)값을 갖는 다중의 입력을 하나의 2진수 값으로 출력하는 모델

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 19
퍼셉트론을 가지고 논리회로를 만들기
AND, OR, NOT GATE등을 만들어 확인
NAND를 가지고 퍼셉트론을 다룰수 있는 함수를 만들 수 있음

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 20
> jupyter notebook
jupyter으로 그래프 그리는 것부터 시작

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 21
고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 22
x = np.array([x1, x2])
w = np.array([0.5, 0.5])
tmp = np.sum(w*x)

np.sum에 대해서 알아볼 것
퍼셉트론을 가지고 아주 간단한 AND 함수를 만들기
퍼셉트론을 가지고 아주 간단한 OR 함수를 만들기
퍼셉트론을 가지고 아주 간단한 NAND 함수를 만들기

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 23
1957년도에는 3가지만 가지고 다 될 줄 알았다.
그런데 문제는 XOR를 해결할 수가 없었다.

XOR문제는 두 개가 서로 다를때만 결과가 1이 되고, 같으면 결과가 0이 되는 논리 게이트를 퍼셉트론에서는 해결 할 수가 없다.

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 24
AND, OR, NAND는 퍼셉트론으로 가능, 달라지는 값은 가중치
가중치만 다르면서 3가지로 동작 가능

퍼셉트론으로는 XOR을 만들 수 없다 ==> 퍼셉트론의 한계
들어오는 두 개의 값이 다를 때만 1을 가진다.

XOR를 해결 할 수 있는 방법을 찾게 됨


0.5*X1 + 0.5*X2 - 0.7 > 0
0.5*X1 + 0.5*X2 > 0.7

0.5*X2 > -0.5*X1 + 0.7

X2 > -X1 + 1.4
결국 y = -x + 1.4 그래프를 그릴 수 있게 됨
퍼셉트론(AND, OR, NAND)은 직선을 하나 그어서 양쪽으로 나눌수 있느냐 없느냐의 문제
XOR는 직선으로 양쪽을 나눌수 없음
(0,1), (1,0) ==> 1
(0,0), (1,1) ==> 0

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 25
과연 XOR문제를 어떻게 해결할 것인가?
==> MLP로 XOR문제 해결

XOR를 AND, OR, NAND를 합성해서 해결 (다중 퍼셉트론, hidden layer)
다중 layer로 퍼셉트론을 해결할 수 있다.

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 26
XOR를 다중 layer를 이용하면 다양한 방법으로 해결할 수 있다.
1) NAND gate만으로도 해결 가능
2) AND, OR, NAND를 합성해서 해결 가능

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 27
퍼셉트론을 가지고 프로그래밍하는 시간(XOR)
X0 X1 Y0
0  0  0
0  1  1
1  0  1
1  1  0

입력 값이 10000개를 해도 상관없다. hidden layer에 어떤 값이 들어오든 상관없다

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 28 (Step2_Percentron/MLPerceptron.py)

def MLPerceptron(X,W1,W2):
    x = np.array(X)
    w1 = np.array(W1)
    w2 = np.array(W2)
    s1 = np.array([0.0]*len(X))
    s2 = np.array([0.0]*len(X))
    for num in range(len(x)):
        s1[num] = (np.sum(x*w1[num]) > 0)
    for num in range(len(x)):
        s2[num] = np.sum(s1*w2[num]) > 0
    return s2
W1 = [[-0.5,-0.5,0,0,0,0.7],
      [0.5,0.5,0,0,0,-0.2],
      [0,0,0,0,0,0],
      [0,0,0,0,0,0],
      [0,0,0,0,0,0],
      [0,0,0,0,0,1]]

W2 = [[0.5,0.5,0,0,0,-0.7],
      [0,0,0,0,0,0],
      [0,0,0,0,0,0],
      [0,0,0,0,0,0],
      [0,0,0,0,0,0],
      [0,0,0,0,0,0]]

W1의 값은 x0 = -0.5, x1 = -0.5, x2 = 0, x3 = 0, x4 = 0, x5 = 0.7
hidden layer로 첫 진입시 계산에 필요한 값
W2는 W1에서 계산된 값을 다음 hidden layer로 넘길때 사용하는 값

W1, W2의 값이 왜 그렇게 되는지 확인해 볼 것

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 29
W1과 W2 값만 찾아주면 perceptron이 알아서 찾아줌
s1은 첫번째 은닉층, s2는 두번째 은닉층

x * w1[num]을 각 곱해서 모두 더함
ex) x *w1[0] => [-0.  -0.   0.   0.   0.   0.7]
    np.sum => 0.7
    s1[num] = (np.sum(x*w1[num]) > 0)
    s1[0]에는 0보다 크니 1을 넣어줌

이렇게 s1을 만들고,  s2는 s1의 결과값을 가지고 함
x[0], x[1]값에 (0, 0), (1,0), (0, 1), (1,1)을 바꿔가면서 넣어줌,나머지는 의미없음

해서 최종 결과가 s2[0] 값만 사용하면 됨


고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 30
다중 퍼셉트론 실습
퍼셉트론은 사용자가 값을 만들어서 넘겨줘야함 (W)
신경망에서는 값을 자동으로 만들어줌

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 31
신경망 이론을 학습
활성화 함수를 이용한다.

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 32 (Step3_deeplearning/stepfunction.py)
활성화함수 : 입력신호의 총합을 출력신호로 변환하는것
계단함수, 시그모이드, Relu

계단함수, 시그모이드 -> 성능 저하
Relu -> 성능 좋음, 최근에 많이 사용됨

계단함수

계단함수:step function
입력이 0.1, 0.5, -0.4 라면 총합은 0.2가 된다.
출력은 총합이 0보다 작으나 같으면 0으로, 0보다 크면 1로 출력

h(x) = 0 {x가 0이거나 0보다 작을때}
h(x) = 1 {x가 0보다 클때}
입력의 총합 0.2가 활성화함수 h를 거치면 출력은 1이 된다
h(0.2) = 1


"""
import numpy as np
import matplotlib.pylab as plt

def step_function(x):
"""
x>0 참으로 판명
x<=0 거짓으로 판명
참과 거짓을 리턴하는데, dtype=np.int로 그걸 0,1로 바꾸겠다.
"""
    return np.array(x>0, dtype=np.int)
"""
최소값은 -5.0, 최댓값 5.0, 0.1씩 증가
-5.0부터 시작해서 +4.9까지 증가됨
x가 0.1일때 y는 1이 되므로, 그래프가 변경됨
"""
x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)

plt.plot(x,y)
plt.ylim(-0.1, 1.1)
plt.show()
"""

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 33
시그모이드 함수
계단함수가 0에서 불연속(0에서 갑자기 1로 변함)인 점을 보완
S자 형태의 함수로 특징은
1) 끊어지는 부분이 없고
2) 모든 부분에서 미분이 가능하고
3) 미분결과가 사용하기 간편한 형태


"""
import numpy as np
import matplotlib.pylab as plt

def sigmoid_function(x):
	return 1 / (1+np.exp(-x))
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid_function(x)
plt.plot(x,y)
plt.ylim(-0.1, 1.1)
plt.show()
"""
0에서 부드럽게 이어진다. x가 작아지면 작아질수록 0으로 가고, 커지면 1로 간다.
x가 0에 가까우면 작아지고, 0보다 멀어지면 커지고
계단함수보다 좀 더 효율적이다.

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 34
Relu 함수
입력이 0이하이면 0을 출력, 0이상이면 입력값을 그대로 출력

"""
import numpy as np
import matplotlib.pylab as plt

def relu_function(x):
	return np.maximum(0,x)
x = np.arange(-5.0, 5.0, 0.1)
y = relu_function(x)
plt.plot(x,y)
plt.ylim(-5, 5)
plt.show()
"""

선형과 비선형
선형은 y = ax+b밖에 없다.
그외의 나머지는 비선형

선형은 풀기가 쉽다. 비선형은 어렵다.
비선형의 문제들도 가능한 선형으로 만들려고한다. -> 선형은 해법이 있기 때문에
비선형은 아주 특별한 경우를 제외하고는 해법을 찾기 어렵다.
활성화 함수로 선형을 사용하면?
y = ax+b
w = cy+d
-> w = c(ax+b) + d
-> w = acx + bc + d
-> ac = A, bc+d = B 로 두면 , w = Ax + B --> y= ax+b 와 동일한 모습

여러번 해도 동일해도 그 모습 그대로 유지한다 -> 문제를 쉽게 풀수 있다.
단, XOR 문제는 해결하지 못한다.

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 35
Relu를 사용하는 이유
시그모이드는 hidden layer를 많이 거칠수록 0.00000으로 간다..0으로 딱 가지지 않는다.
깊어지면 깊어질수록 정확도가 떨어진다. 시그모이드의 약점
그래서 Relu가 만들어짐

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 36
행렬 공부

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 37
A H1
B H2
C H3
W는 가중치값,

H1 = W1A + W2A + W3A
H2 = W4B + W5B + W6B
H3 = W7C + W8C + W9C

행렬로 나타내면 다음과 같음

H1 = (W1 W2 W3) (A)
H2 = (W4 W5 W6) (B)
H3 = (W7 W8 W9) (c)

Numpy를 이용해서 행렬 계산 가능

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 38
고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 39
다차원 배열 계산
"""
import numpy as np
X = np.array([0, 0, 1])

W1 = np.array([[2,2,-1], [2,2,0], [0, -1, -1]])
W2 = np.array([[2,2,-1], [-1, 0, 0], [2, -1, 2]])
W3 = np.array([[2, -1, 0], [1, -1, 0], [2, 0, 0]])

"""
Y와 Y1..행렬 결합법칙 확인
"""
Y = np.dot(np.dot(np.dot(W3, W2), W1), X)

Y1 = np.dot(W3, np.dot(W2, np.dot(W1, X)))

print(Y, Y1)
"""

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 40
고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 41
고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 42
고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 43
고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 44
numpy를 이용해서 sigmod 구현 및 1층에서 2층으로
2층에서 3층으로, 3층에서 출력층으로
출력에서는 그대로 력(identity_function)
"""
import numpy as np

def sigmoid(x):
    return 1 / (1+np.exp(-x))
# 1층 계산
X = np.array([1.0, 0.5]) #2
W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) #2x3
B1 = np.array([0.1, 0.2, 0.3])

A1 = np.dot(X, W1) + B # 1 * 1x3 -> 1x3 행렬 출력
					   # 1x3 + 1x3 -> 1x3
Z1 = sigmoid(A1)
print(Z1)
# 2층에서 2층으로
W2 = np.array([[0.1, 0.4],[0.2, 0.5], [0.3, 0.6]]) # 3x2
B2 = np.array([0.1, 0.2]) #2

A2 = np.dot(A1, W2) + B2 # (1x)3 * 3x2 >> (1x)2
Z2 = sigmoid(A2)
print(A2, Z2)


#3층 계산
def identity_function(x):
    return x

W3 = np.array([[0.1, 0.3],[0.2, 0.4]])
B3 = np.array([0.1, 0.2])

A3 = np.dot(Z2, W3) + B3

Y = identity_function(A3)
#출력
print(A3, Y)
"""

출력층의 활성화 함수는 문제에 따라 정한다. 일반적으로 회귀에는 항등 함수를, 2클래스 분류에는 시그모이드 함수 사용
하지만, 최근에는 ReLu 함수가 사용되면서 주로 ReLu 함수를 사용하게 된다.

또 다른 실습
"""
import numpy as np

def sigmoid(x):
    return 1 / (1+np.exp(-x))

def identity_function(x):
    return x

# 사전으로 만들어서 key, value로 만듬
def init_network():
    network = {}
    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
    network['b1'] = np.array([0.1, 0.2, 0.3])
    network['W2'] = np.array([[0.1, 0.4],[0.2, 0.5], [0.3, 0.6]])
    network['b2'] = np.array([0.1, 0.2])
    network['W3'] = np.array([[0.1, 0.3],[0.2, 0.4]])
    network['b3'] = np.array([0.1, 0.2])
    return network

def forward(network, x):
    W1, W2, W3 = network['W1], network['W2'], network['W3']
	b1, b2, b3 = network['b1], network['b2'], network['b3']
	a1 = np.dot(x, W1)+ b1
	z1 = sigmoid(a1)

	a2 = np.dot(a1, W2) + b2
	z2 = sigmoid(a2)

	a3 = np.dot(z2, W3) + b3
	y = identity_function(a3)
	return y
network = init_network()
x = np.array([1.0, 0.5])
y = forward(network, x)

print(y)
"""

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 45
고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 46
결과값을 한번 더 바꿔주는 방법 : 항등함수 사용 or 아래 방법 사용
- 소프트맥스
- 지수함수

위 함수를 사용해서 확률로 바꾸는 경우
y1, y2, y3에 각각 저장된 값을 위 방법을 사용해서 변환하는것
1, 7, 2의 결과값이 나왔을 때, 1+7+2 = 10
1/10 7/10, 2/10의 확률로 바꿈

결과 값이 10과 70과 20일 경우에는 어떻게 될까? --> 똑같은 확률을 가지게 됨
0.01, 0.07, 0.02 일때는??
여러개의 은닉층을 거져서 출력값이 굉장히 작은 수가 나온다. --> 큰 의미가 없는 가능성

다 더해서 계산하지 않고,
0.01, 0.07, 0.02가 있고, 1, 7, 2가 있고, 10, 70, 20이 있을 때
지수함수를 이용
e에 0.01승, e에 0.07승....으로 변환을 계산을 해보자
1,7,2 를 e(지수함수)를 적용해보면 7이 1102로 되어서 확률이 커짐

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 47
소프트맥스
1,7,2에 대한 지수 함수 를 구해서 다 더해서 분모를 만들고 각 값을 분자로 만들어서 확률 계산

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 50~57
MNIST 이론 및 실습

학습할 자료와 테스트할 자료를 나눠놓을 필요가 있다.
학습용 data : 60000개
test용 data : 10000개

배치처리
IO에 상대적으로 많은 시간 소요
메모리가 허용하는 한 한 번에 많은 데이터를 읽어옴
1장씩 사진을 처리하면 1장씩 10000번 읽어와야하지만
100장식 사진을 처리하면 100장씩 100번을 읽어오게 됨
즉, 1장씩 1만번 읽기를 하는 것 보다 100장씩 100번 반복에 걸리는 시간이 짧음
큰 배열을 계산하는 것이 작은 배열을 여러번 계산하는 것보다 빠름
""" 100번 돌리기
x, t = get_data()
network = init_network()

batch_size = 100
accuracy_cnt = 0
for i in range(0, len(x), batch_size):
    x_batch = x[i:i+batch_size]
    y_batch = predict(network,x_batch)
    p = np.argmax(y_batch, axis=1)
    accuracy_cnt+=np.sum(p==t[i:i+batch_size])

print("Accuracy:" + str(float(accuracy_cnt / len(x))))
"""

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 58~61
Loss Function - 정답하고 가까운지 먼지 찾는 방법

평균 제곱 오차(Mean Square Error, MSE)
MSE가 작을 수록 추정의 정확성이 높아짐
w값을 변경해가면서 MSE 결과값을 비교해가면서 이전과 비교

MST값도 1/10로 줄어듬, 오차를 줄여가는 과정이 학습이 이루어지는 과정
w값과 b값을 계속해서 바꿔줌
--> w와 b값을 작게 바꿔줬더니 오차가 작아졌다 --> 올바른 방향이구나
계속해서 진행하자

교차엔트로피 오차(Cross Entropy Error, CEE)

t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
y1 = [.1, .05, .1, .0, .05, .1, .0, .6, .0, .0]
y4 = [.1, .05, .1, .0, .05, .1, .0, .0, .7, .0]
y5 = [.1, .05, .1, .0, .05, .1, .0, .4, .2, .0]
y57 = [.1, .05, .6, .0, .05, .1, .0, .1, .2, .0]

결과값 y를 w, b를 바꿔가면서 계산
평균 제곱 오차는 실제로 2일 확률은 변하지 않고, 다른 값의 확률만 변경되는 경우가 있음
y1, y4, y5에서 실제 2값은 0.1로 동일하다. 다른 값이 변하면서 오차값이 커지기도 하고, 작아지기도 함
-> 학습할때 잘못된 가능성이 있음

엔트로피 -> 필요없는 정보들을 최대한 제거해주는 것(노이즈 제거)

내가 원하는 정보는 2일 확률...즉, 2일 확률이 높아야 한다.
2일 확률을 제외하고 나머지 확률은 다루지 않겠다. 나머지는 엔트로피로 처리하겠다.

평균 제곱 오차는 계산량이 많다. 다 더해서 빼고, N으로 나눠야하기 때문에
하지만 엔트로피는 특정 값만 계산하기때문에 계산량이 많지 않다.(정답이 0일때는 계산을 하지 않는다.)
--> 교차 엔트로피 오차를 많이 사용한다.

불필요한 것은 과감하게 버리고, 필요한 것만 취하자.

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 - TIP 1
argmax() - python numpy 함수, 가장 큰 값의 위치 반환
https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html

axis = 0 , 0이 들어갈 때는 열중에서 가장 큰 값의 자리 반환

a = np.arange(6).reshape(2,3) +10
print(a)
np.argmax(a, axis=0)

[[10 11 12]
 [13 14 15]]

array([1, 1, 1]) // 13, 14, 15 값이 크니까 그 위치 반환

np.argmax(a, axis=1), axis = 1이면 같은 행 중에서 가장 큰 값을 찾고, 위치 반환
array([2, 2])

고등학교 수학만 알면 따라할 수 있는 인공지능, 머신러닝, 딥러닝 62
Loss Function
log() 함수